ann
üîπ Basic Understanding
1.	What is an Artificial Neural Network (ANN)?
An ANN is a computational model inspired by the human brain, consisting of layers of interconnected neurons used for pattern recognition, classification, regression, etc.
2.	What are the main components of an ANN?
o	Neurons (nodes)
o	Layers (input, hidden, output)
o	Weights and biases
o	Activation functions
3.	What is the difference between perceptron and multilayer perceptron?
o	Perceptron: A single-layer neural network (only input and output).
o	Multilayer Perceptron (MLP): Has one or more hidden layers, allowing it to solve non-linear problems.
________________________________________
üîπ Technical Concepts
4.	What is forward propagation?
It‚Äôs the process of passing inputs through the layers of the network to generate the output.
5.	What is backpropagation?
A learning algorithm that adjusts the weights in the network by propagating the error backward from the output to the input layer.
6.	What is the activation function, and why is it used?
It introduces non-linearity to the model, enabling the ANN to learn complex patterns. Examples: ReLU, Sigmoid, Tanh.
7.	What are weights and biases?
o	Weights determine the importance of input features.
o	Biases allow the model to shift the activation function for better fitting.
________________________________________
üîπ Training & Performance
8.	What is overfitting in ANN?
When the model learns the training data too well, including noise, and performs poorly on new/unseen data.
9.	How can you prevent overfitting in ANN?
o	Use dropout
o	Early stopping
o	Regularization (L1/L2)
o	Cross-validation
10.	What is the role of the learning rate?
It controls how much the weights are adjusted during training. A high learning rate might overshoot the minimum; a low one might be too slow.
________________________________________
üîπ Practical and Code Related
11.	Which libraries are commonly used to implement ANN in Python?
o	TensorFlow
o	Keras
o	PyTorch
o	Scikit-learn (for simple MLP models)
12.	Can you explain the structure of the ANN you implemented in your practical?
(Be ready to describe the number of layers, neurons, activation functions used, and dataset.)
13.	What loss function did you use and why?
o	Classification: categorical_crossentropy or binary_crossentropy
o	Regression: mean_squared_error
14.	What optimizer did you use?
Common ones: SGD, Adam, RMSprop.
Adam is popular due to its efficiency and faster convergence.
15.	What is an epoch and a batch?
o	Epoch: One full pass through the entire training data.
o	Batch: A subset of the data used to train the model at one time.
________________________________________
2. Classifier and Regressor using ANN (CO II, III, IV)
Q3. What‚Äôs the difference between ANN for classification and regression?
A: Classification predicts categories (e.g., spam or not), regression predicts continuous values (e.g., house prices).
Q4. What activation function is used in classification vs regression?
A:
‚Ä¢	Classification: ReLU in hidden layers, softmax or sigmoid in output.
‚Ä¢	Regression: ReLU or linear in output.
Q5. What is a loss function and which ones are commonly used?
A: Measures error.
‚Ä¢	Classification: Binary/Categorical Crossentropy
‚Ä¢	Regression: Mean Squared Error (MSE)
Q6. What is a confusion matrix?
A: A table showing TP, TN, FP, FN used to evaluate classification accuracy.
Q7. Define precision, recall, F1-score.
A:
‚Ä¢	Precision = TP / (TP + FP)
‚Ä¢	Recall = TP / (TP + FN)
‚Ä¢	F1 = Harmonic mean of precision & recall
Q8. How do you visualize the training performance of an ANN?
A: Using a loss graph or accuracy graph over epochs.


Navie bayes
Here are detailed viva questions and answers for the Bayesian Classifier / Naive Bayes topic in your ML practical:
________________________________________
‚úÖ Bayesian Classifier / Naive Bayes ‚Äì Viva Questions
Q1. What is a Bayesian classifier?
A: It‚Äôs a probabilistic model based on Bayes‚Äô Theorem that predicts the class of a sample using prior knowledge and evidence from the data.
________________________________________
Q2. State Bayes‚Äô Theorem.
A:
P(A‚à£B)=P(B‚à£A)‚ãÖP(A)P(B)P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} 
Where:
‚Ä¢	P(A‚à£B)P(A|B): Posterior probability
‚Ä¢	P(B‚à£A)P(B|A): Likelihood
‚Ä¢	P(A)P(A): Prior
‚Ä¢	P(B)P(B): Evidence
________________________________________
Q3. What is Naive Bayes? Why is it ‚Äúnaive‚Äù?
A: Naive Bayes is a classification technique assuming all features are independent given the class label, which is rarely true‚Äîhence the term "naive".
________________________________________
Q4. What are the types of Naive Bayes classifiers?
A:
‚Ä¢	Gaussian Naive Bayes: For continuous data (assumes normal distribution)
‚Ä¢	Multinomial Naive Bayes: For count data (e.g., text classification)
‚Ä¢	Bernoulli Naive Bayes: For binary features (e.g., presence/absence)
________________________________________
Q5. What are the advantages of Naive Bayes?
A:
‚Ä¢	Fast and simple
‚Ä¢	Performs well on high-dimensional data (e.g., spam detection)
‚Ä¢	Works with small datasets
________________________________________
Q6. What are the limitations?
A:
‚Ä¢	Assumes feature independence (rare in real-world data)
‚Ä¢	Poor performance if features are highly correlated
________________________________________
Q7. How is probability calculated in Naive Bayes for a given class?
A:
P(C‚à£X)=P(X1‚à£C)‚ãÖP(X2‚à£C)‚ãÖ...‚ãÖP(Xn‚à£C)‚ãÖP(C)P(X)P(C|X) = \frac{P(X_1|C) \cdot P(X_2|C) \cdot ... \cdot P(X_n|C) \cdot P(C)}{P(X)} 
________________________________________
Q8. What is Laplace smoothing and why is it used?
A: It avoids zero probabilities for unseen features by adding a small constant (usually 1) to all counts.
________________________________________
Q9. What metrics are used to evaluate a Naive Bayes classifier?
A:
‚Ä¢	Accuracy
‚Ä¢	Precision, Recall, F1-score
‚Ä¢	Confusion Matrix
________________________________________
Q10. Where is Naive Bayes commonly used?
A:
‚Ä¢	Email spam detection
‚Ä¢	Sentiment analysis
‚Ä¢	Document classification
‚Ä¢	Disease prediction
________________________________________
Let me know if you want code + output for a Naive Bayes classifier too!


Pac
Here are important viva questions and answers related to the PAC (Probably Approximately Correct) learning framework, which is a theoretical concept in machine learning.
________________________________________
‚úÖ PAC Learning ‚Äì Viva Questions and Answers
________________________________________
Q1. What is PAC learning?
A: PAC stands for Probably Approximately Correct learning. It‚Äôs a theoretical framework introduced by Leslie Valiant to study the learnability of functions. A hypothesis class is PAC-learnable if, with high probability, it can learn a hypothesis that is approximately correct.
________________________________________
Q2. What does "Probably Approximately Correct" mean?
A: It means:
‚Ä¢	Probably (1 - Œ¥): The learning algorithm returns a good hypothesis with high probability (confidence).
‚Ä¢	Approximately (Œµ): The hypothesis has a small error (is close to the true function).
Formally, a learner returns a hypothesis with error ‚â§ Œµ with probability ‚â• 1 - Œ¥.
________________________________________
Q3. What are Œµ (epsilon) and Œ¥ (delta) in PAC learning?
A:
‚Ä¢	Œµ (epsilon): Error tolerance ‚Äî how close the hypothesis should be to the target concept.
‚Ä¢	Œ¥ (delta): Confidence ‚Äî how likely the algorithm is to succeed. Smaller Œ¥ = higher confidence.
________________________________________
Q4. What are the assumptions of PAC learning?
A:
‚Ä¢	The training examples are drawn i.i.d. (independently and identically distributed).
‚Ä¢	The hypothesis belongs to a known hypothesis class.
‚Ä¢	There is a target concept to approximate.
________________________________________
Q5. What is sample complexity in PAC learning?
A: It's the number of training examples needed to ensure the algorithm learns an approximately correct hypothesis with high probability.
For binary classification, sample complexity is:
m‚â•1Œµ(ln‚Å°‚à£H‚à£+ln‚Å°1Œ¥)m ‚â• \frac{1}{\varepsilon} \left( \ln |H| + \ln \frac{1}{\delta} \right) 
Where:
‚Ä¢	mm = number of samples
‚Ä¢	‚à£H‚à£|H| = size of hypothesis space
‚Ä¢	Œµ\varepsilon, Œ¥\delta as defined above
________________________________________
Q6. Is Naive Bayes PAC-learnable?
A: Yes, under certain conditions (independence of features and finite hypothesis class), Naive Bayes can be PAC-learnable.
________________________________________
Q7. What are the limitations of PAC learning?
A:
‚Ä¢	Assumes access to a perfect hypothesis class
‚Ä¢	May not work well with noisy or real-world data without adjustments
‚Ä¢	Doesn't handle adversarial inputs
________________________________________
Q8. What are the variations of PAC learning?
A:
‚Ä¢	Agnostic PAC learning: Does not assume the existence of a perfect hypothesis
‚Ä¢	Robust PAC learning: Accounts for adversarial noise
‚Ä¢	Online PAC learning: Learns from a stream of data points
________________________________________
Q9. How is PAC learning related to generalization?
A: PAC learning guarantees that the learned hypothesis generalizes well to unseen data with high probability.
________________________________________
Q10. Why is PAC learning important in ML theory?
A: It provides mathematical guarantees for learning, helps analyze feasibility and efficiency of algorithms, and forms the foundation of computational learning theory.
________________________________________
Let me know if you'd like a summary slide, PDF, or a diagram explaining the PAC learning framework!


 Decision tree
Q1. What is a Decision Tree?
A: A tree-like structure where internal nodes represent features, branches represent conditions, and leaves represent outcomes.
Q2. How is it used for classification vs regression?
A:
‚Ä¢	Classification: predicts a category
‚Ä¢	Regression: predicts a numeric value
Q3. What are splitting criteria in classification?
A:
‚Ä¢	Gini Impurity
‚Ä¢	Entropy (Information Gain)
Q4. What is overfitting in decision trees?
A: When the model is too deep and learns noise in training data. Use pruning or max_depth to reduce it.
Here are detailed viva questions and answers for Decision Tree Classification and Regression, useful for your Machine Learning practicals:
________________________________________
‚úÖ Decision Tree ‚Äì Viva Questions and Answers
________________________________________
üìå BASICS
Q1. What is a Decision Tree?
A: A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It splits the data into branches based on feature values to make predictions.
________________________________________
Q2. What is the basic idea behind a decision tree?
A: The algorithm splits the dataset into subsets based on feature values to form a tree-like structure. Each internal node represents a feature, each branch a decision, and each leaf a predicted outcome.
________________________________________
üìå TYPES
Q3. What is the difference between Decision Tree Classification and Regression?
A:
Feature	Classification	Regression
Output	Class Label (categorical)	Continuous Value
Splitting Criteria	Gini, Entropy	MSE, MAE
Example Task	Spam detection, diagnosis	Price prediction, forecasting
________________________________________
üìå ALGORITHMS & CRITERIA
Q4. What are common algorithms to build decision trees?
A:
‚Ä¢	ID3 ‚Äì Uses Entropy & Information Gain
‚Ä¢	CART ‚Äì Uses Gini Index (Classification) or MSE (Regression)
‚Ä¢	C4.5 / C5.0 ‚Äì Improved versions of ID3
________________________________________
Q5. What is Gini Index?
A: A measure of impurity or diversity used in CART.
Gini=1‚àí‚àëi=1Cpi2Gini = 1 - \sum_{i=1}^{C} p_i^2 
Where pip_i is the probability of class ii.
________________________________________
Q6. What is Entropy and Information Gain?
A:
‚Ä¢	Entropy measures disorder in the data.
Entropy=‚àí‚àëpilog‚Å°2piEntropy = - \sum p_i \log_2 p_i 
‚Ä¢	Information Gain = Reduction in entropy after a split.
________________________________________
Q7. What is the criterion used in regression trees?
A:
‚Ä¢	Mean Squared Error (MSE)
‚Ä¢	Mean Absolute Error (MAE)
________________________________________
üìå ADVANTAGES & LIMITATIONS
Q8. What are the advantages of Decision Trees?
A:
‚Ä¢	Easy to interpret
‚Ä¢	Handles both numerical and categorical data
‚Ä¢	No feature scaling needed
‚Ä¢	Works well with missing values
________________________________________
Q9. What are the limitations?
A:
‚Ä¢	Prone to overfitting
‚Ä¢	Sensitive to data variations
‚Ä¢	Biased with unbalanced datasets
________________________________________
Q10. How to avoid overfitting in decision trees?
A:
‚Ä¢	Pruning (Pre-pruning or Post-pruning)
‚Ä¢	Limiting tree depth
‚Ä¢	Setting min samples per split
________________________________________
üìå EVALUATION
Q11. What metrics are used to evaluate a classification tree?
A:
‚Ä¢	Accuracy
‚Ä¢	Confusion Matrix
‚Ä¢	Precision, Recall, F1-score
________________________________________
Q12. What metrics are used for regression trees?
A:
‚Ä¢	RMSE (Root Mean Squared Error)
‚Ä¢	MAE (Mean Absolute Error)
‚Ä¢	R¬≤ Score (Coefficient of determination)
________________________________________
üìå APPLICATIONS
Q13. Where are decision trees used?
A:
‚Ä¢	Credit scoring
‚Ä¢	Medical diagnosis
‚Ä¢	Price prediction
‚Ä¢	Fraud detection
‚Ä¢	Customer churn prediction

Svm

Here are viva questions and answers on SVM (Support Vector Machine) tailored for your Machine Learning practical:
________________________________________
‚úÖ SVM (Support Vector Machine) ‚Äì Viva Questions and Answers
________________________________________
üìå BASICS
Q1. What is an SVM?
A: SVM (Support Vector Machine) is a supervised learning algorithm used for classification and regression. It finds the optimal hyperplane that separates data points of different classes with the maximum margin.
________________________________________
Q2. What is a hyperplane in SVM?
A: A hyperplane is a decision boundary that separates different classes. In 2D, it‚Äôs a line; in 3D, it‚Äôs a plane; and in higher dimensions, it‚Äôs a generalization.
________________________________________
Q3. What are support vectors?
A: Support vectors are the data points closest to the hyperplane. They determine the position and orientation of the hyperplane. The SVM aims to maximize the margin between these points and the hyperplane.
________________________________________
üìå MARGINS & KERNELS
Q4. What is the margin in SVM?
A: The margin is the distance between the hyperplane and the nearest support vectors from both classes. SVM maximizes this margin to improve generalization.
________________________________________
Q5. What is the difference between hard margin and soft margin SVM?
A:
‚Ä¢	Hard Margin: No misclassifications allowed; only works if data is linearly separable.
‚Ä¢	Soft Margin: Allows some misclassification; better for noisy or overlapping data.
________________________________________
Q6. What is the kernel trick in SVM?
A: The kernel trick allows SVM to transform data into higher dimensions where it may become linearly separable. It avoids computing the transformation explicitly by using kernel functions.
________________________________________
Q7. What are common kernel functions in SVM?
A:
‚Ä¢	Linear Kernel: For linearly separable data
‚Ä¢	Polynomial Kernel: For curved boundaries
‚Ä¢	RBF (Radial Basis Function) / Gaussian Kernel: For complex boundaries
‚Ä¢	Sigmoid Kernel: Similar to neural networks
________________________________________
üìå APPLICATION & PERFORMANCE
Q8. What are the advantages of SVM?
A:
‚Ä¢	Works well for high-dimensional data
‚Ä¢	Effective when number of features > number of samples
‚Ä¢	Robust to overfitting in many cases
________________________________________
Q9. What are the limitations of SVM?
A:
‚Ä¢	Not suitable for large datasets (high training time)
‚Ä¢	Poor performance on overlapping classes
‚Ä¢	Needs careful tuning of kernel parameters
________________________________________
Q10. What are the important parameters in SVM?
A:
‚Ä¢	C (Regularization parameter): Controls trade-off between achieving low error and margin maximization
‚Ä¢	Gamma (for RBF kernel): Defines how far the influence of a single training example reaches
‚Ä¢	Kernel type
________________________________________
üìå EVALUATION
Q11. What metrics are used to evaluate SVM classification?
A:
‚Ä¢	Accuracy
‚Ä¢	Confusion Matrix
‚Ä¢	Precision, Recall, F1 Score
‚Ä¢	ROC-AUC curve
________________________________________
üìå MISCELLANEOUS
Q12. Can SVM be used for regression?
A: Yes, Support Vector Regression (SVR) is a variant of SVM that predicts continuous values.
________________________________________
Q13. What is the cost function of SVM?
A: It tries to minimize:
12‚à£‚à£w‚à£‚à£2+C‚àëi=1nŒæi\frac{1}{2} ||w||^2 + C \sum_{i=1}^{n} \xi_i 
Where:
‚Ä¢	‚à£‚à£w‚à£‚à£||w||: margin
‚Ä¢	Œæi\xi_i: slack variables (for soft margin)
‚Ä¢	CC: regularization parameter
________________________________________
Q14. What is overfitting in SVM and how do you prevent it?
A: Overfitting occurs when the model fits noise. Prevented by:
‚Ä¢	Choosing the right C and gamma values
‚Ä¢	Using cross-validation
‚Ä¢	Reducing feature dimensionality
________________________________________
Let me know if you'd like a Python example using sklearn.svm.SVC or an SVM loss graph visualization!


K mean
Here are detailed viva questions and answers for K-Means Clustering, formatted for your ML practicals:
________________________________________
‚úÖ K-Means Clustering ‚Äì Viva Questions and Answers
________________________________________
üìå BASICS
Q1. What is K-Means clustering?
A: K-Means is an unsupervised learning algorithm used to partition data into K clusters, where each data point belongs to the cluster with the nearest centroid (mean of the cluster).
________________________________________
Q2. Is K-Means a supervised or unsupervised algorithm?
A: It is an unsupervised algorithm, because it works on unlabeled data and finds inherent patterns or groupings in the dataset.
________________________________________
Q3. What is the goal of K-Means?
A: To minimize the within-cluster sum of squares (WCSS), which is the sum of squared distances between each point and its assigned cluster centroid.
________________________________________
üìå WORKING
Q4. What are the steps in the K-Means algorithm?
A:
1.	Choose the number of clusters K
2.	Randomly initialize K centroids
3.	Assign each point to the nearest centroid
4.	Recalculate the centroids of each cluster
5.	Repeat steps 3‚Äì4 until centroids stop changing (convergence)
________________________________________
Q5. How is the distance between points and centroids calculated?
A: Most commonly using Euclidean distance.
Distance=(x1‚àíy1)2+(x2‚àíy2)2+...\text{Distance} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ...} 
________________________________________
üìå PARAMETERS AND EVALUATION
Q6. How do you choose the value of K?
A:
‚Ä¢	Elbow Method: Plot WCSS vs. K and look for the ‚Äúelbow‚Äù point
‚Ä¢	Silhouette Score: Measures how similar a point is to its own cluster vs. other clusters
‚Ä¢	Domain knowledge
________________________________________
Q7. What is inertia in K-Means?
A: Inertia is the sum of squared distances between data points and their cluster centroids. Lower inertia generally indicates better clustering.
________________________________________
Q8. What is the cost function of K-Means?
A:
J=‚àëi=1k‚àëxj‚ààSi‚à£‚à£xj‚àíŒºi‚à£‚à£2J = \sum_{i=1}^{k} \sum_{x_j \in S_i} ||x_j - \mu_i||^2 
Where:
‚Ä¢	SiS_i = cluster
‚Ä¢	Œºi\mu_i = centroid of cluster
‚Ä¢	xjx_j = data points
________________________________________
üìå LIMITATIONS
Q9. What are the disadvantages of K-Means?
A:
‚Ä¢	Requires you to predefine K
‚Ä¢	Sensitive to initialization
‚Ä¢	Doesn‚Äôt work well with non-spherical or overlapping clusters
‚Ä¢	Affected by outliers
________________________________________
Q10. What are some ways to improve K-Means?
A:
‚Ä¢	Use K-Means++ for better centroid initialization
‚Ä¢	Try multiple runs with different initial seeds
‚Ä¢	Remove outliers beforehand
‚Ä¢	Normalize/scale the data
________________________________________
üìå APPLICATIONS
Q11. What are real-life applications of K-Means?
A:
‚Ä¢	Customer segmentation
‚Ä¢	Image compression
‚Ä¢	Market basket analysis
‚Ä¢	Anomaly detection
________________________________________
Q12. What is K-Means++? How is it different?
A: K-Means++ improves centroid initialization by spreading out initial centroids. This reduces the chances of poor clustering and improves convergence speed.
________________________________________
üìå MISCELLANEOUS
Q13. Is K-Means deterministic?
A: No, since centroid initialization is random (unless using K-Means++), results may vary across different runs.
________________________________________
Q14. Can K-Means be used for non-numeric data?
A: Not directly. K-Means relies on distance metrics, so categorical data must be encoded (e.g., one-hot encoding or using K-Modes for categorical features).
________________________________________
Q15. What is the time complexity of K-Means?
A:
‚Ä¢	O(n * k * i * d)
Where:
‚Ä¢	n = number of data points
‚Ä¢	k = number of clusters
‚Ä¢	i = number of iterations
‚Ä¢	d = number of features
________________________________________
Let me know if you‚Äôd like a code example in Python, an Elbow Method plot, or to cover K-Medoids or DBSCAN as well.


Knn

Here are detailed viva questions and answers for K-Nearest Neighbors (KNN) ‚Äì perfect for your Machine Learning practical preparation:
________________________________________
‚úÖ KNN (K-Nearest Neighbors) ‚Äì Viva Questions and Answers
________________________________________
üìå BASICS
Q1. What is KNN?
A: KNN (K-Nearest Neighbors) is a supervised learning algorithm used for classification and regression. It classifies a data point based on the majority class among its 'K' nearest neighbors in the training data.
________________________________________
Q2. Is KNN supervised or unsupervised?
A: It is a supervised algorithm because it uses labeled data to make predictions.
________________________________________
Q3. How does KNN work?
A:
1.	Choose a value for K (number of neighbors).
2.	Calculate the distance between the test point and all training points.
3.	Select the K nearest training samples.
4.	For classification: Assign the most common class.
For regression: Take the mean/average value of neighbors.
________________________________________
üìå DISTANCE METRICS
Q4. What distance metrics are used in KNN?
A:
‚Ä¢	Euclidean distance (most common)
‚Ä¢	Manhattan distance
‚Ä¢	Minkowski distance
‚Ä¢	Hamming distance (for categorical variables)
________________________________________
Q5. What is Euclidean distance?
A:
d(p,q)=‚àëi=1n(pi‚àíqi)2d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2} 
Where p and q are two data points and n is the number of features.
________________________________________
üìå HYPERPARAMETERS AND CHALLENGES
Q6. How do you choose the value of K?
A:
‚Ä¢	Use cross-validation
‚Ä¢	Plot error rate vs. K
‚Ä¢	Avoid very small (overfitting) or very large K (underfitting)
________________________________________
Q7. What happens when K = 1 or very high?
A:
‚Ä¢	K = 1: Model becomes too sensitive (overfits).
‚Ä¢	K = large: Model becomes too generalized (underfits).
________________________________________
Q8. What are the limitations of KNN?
A:
‚Ä¢	Computationally expensive (slow on large datasets)
‚Ä¢	Sensitive to irrelevant features and noise
‚Ä¢	Needs feature scaling
‚Ä¢	Poor performance with high dimensionality
________________________________________
üìå ADVANTAGES
Q9. What are the advantages of KNN?
A:
‚Ä¢	Simple and intuitive
‚Ä¢	No training phase (lazy learner)
‚Ä¢	Works well with small datasets
‚Ä¢	Non-parametric (no assumption about data distribution)
________________________________________
Q10. Is KNN a parametric or non-parametric algorithm?
A: Non-parametric, as it does not make any assumptions about the underlying data distribution.
________________________________________
üìå CLASSIFICATION VS REGRESSION
Q11. How does KNN work for regression?
A: For regression, KNN predicts the average of the values of the K nearest neighbors rather than voting on the class.
________________________________________
Q12. Can KNN be used for both classification and regression?
A: Yes. It can solve both:
‚Ä¢	Classification: Majority vote of neighbors
‚Ä¢	Regression: Mean of neighbors' values
________________________________________
üìå MODEL EVALUATION
Q13. What evaluation metrics are used for KNN?
‚Ä¢	For classification: Accuracy, Confusion Matrix, Precision, Recall, F1-Score
‚Ä¢	For regression: MSE, RMSE, MAE, R¬≤ Score
________________________________________
Q14. Why do we need to scale features in KNN?
A: Because distance metrics are sensitive to the scale of features. Without scaling, larger-scaled features can dominate the distance calculation.
________________________________________
üìå MISCELLANEOUS
Q15. Is KNN a lazy learner or eager learner? Why?
A: Lazy learner ‚Äì It stores the entire training dataset and performs computation at prediction time. It does not build a model beforehand.
________________________________________
Q16. What are some real-world applications of KNN?
A:
‚Ä¢	Recommender systems
‚Ä¢	Handwriting recognition
‚Ä¢	Fraud detection
‚Ä¢	Medical diagnosis (e.g., classifying tumors)
________________________________________
Would you like a Python implementation of KNN using sklearn, or a graph of error rate vs K? I can also help you compare it with other classifiers like Decision Tree or SVM.











Cnn

Here are detailed viva questions and answers for CNN (Convolutional Neural Networks) ‚Äì tailored for your ML practical:
________________________________________
‚úÖ CNN (Convolutional Neural Network) ‚Äì Viva Questions and Answers
________________________________________
üìå BASICS
Q1. What is a CNN?
A: A Convolutional Neural Network (CNN) is a type of deep learning model used mainly for image classification, object detection, and computer vision tasks. It uses convolutional layers to automatically learn spatial hierarchies of features from input images.
________________________________________
Q2. How is CNN different from a regular ANN?
A:
‚Ä¢	ANN: Fully connected, does not consider spatial structure.
‚Ä¢	CNN: Uses local connections, parameter sharing, and pooling to reduce parameters and learn from image pixels.
________________________________________
Q3. Why are CNNs good for image processing?
A:
CNNs capture spatial relationships (e.g., edges, shapes) through convolution filters and preserve spatial locality, which makes them highly effective for image-related tasks.
________________________________________
üìå CNN LAYERS
Q4. What are the main layers in CNN?
A:
1.	Convolutional Layer
2.	Activation Layer (usually ReLU)
3.	Pooling Layer (Max or Average Pooling)
4.	Fully Connected Layer
5.	Output Layer (with Softmax or Sigmoid)
________________________________________
Q5. What is the convolution operation?
A: It is the process of sliding a kernel/filter over the input image and computing the dot product to extract features such as edges or textures.
________________________________________
Q6. What is a kernel or filter in CNN?
A: A small matrix used in the convolution operation to detect features like edges, corners, textures, etc.
________________________________________
Q7. What is the role of ReLU in CNN?
A: ReLU (Rectified Linear Unit) introduces non-linearity to the model and helps prevent vanishing gradients. It replaces all negative values with zero.
________________________________________
Q8. What is pooling in CNN?
A: Pooling reduces the spatial dimensions (width, height) of the feature maps and makes the model invariant to small translations.
Types:
‚Ä¢	Max pooling: Takes the maximum value
‚Ä¢	Average pooling: Takes the average value
________________________________________
üìå TRAINING AND PERFORMANCE
Q9. What is the loss function used in CNN classification?
A: Typically, Categorical Crossentropy for multi-class classification and Binary Crossentropy for binary classification.
________________________________________
Q10. What optimizer is commonly used?
A:
‚Ä¢	Adam (most common)
‚Ä¢	SGD (Stochastic Gradient Descent)
‚Ä¢	RMSProp
________________________________________
Q11. What is overfitting in CNN and how to prevent it?
A: Overfitting happens when the model performs well on training data but poorly on test data.
To prevent it:
‚Ä¢	Use Dropout layers
‚Ä¢	Apply Data Augmentation
‚Ä¢	Use Early stopping
‚Ä¢	Reduce model complexity
________________________________________
Q12. What is dropout?
A: Dropout randomly deactivates some neurons during training to prevent overfitting and improve generalization.
________________________________________
Q13. What does a fully connected (dense) layer do?
A: It connects every neuron in one layer to every neuron in the next. Used at the end of CNN to combine extracted features and perform classification.
________________________________________
üìå APPLICATIONS & MISC
Q14. Where are CNNs used?
A:
‚Ä¢	Image classification (e.g., MNIST, CIFAR)
‚Ä¢	Facial recognition
‚Ä¢	Object detection
‚Ä¢	Medical imaging
‚Ä¢	Self-driving cars
‚Ä¢	Gesture recognition
________________________________________
Q15. Why do we use flattening in CNN?
A: Flattening converts the 2D feature maps into a 1D vector so it can be fed into a fully connected layer for final classification.
________________________________________
Q16. What dataset is commonly used for CNN practice?
A:
‚Ä¢	MNIST (handwritten digits)
‚Ä¢	Fashion-MNIST
‚Ä¢	CIFAR-10
‚Ä¢	ImageNet
________________________________________
Q17. What is the difference between Softmax and Sigmoid?
A:
‚Ä¢	Sigmoid: Used for binary classification; outputs a value between 0 and 1.
‚Ä¢	Softmax: Used for multi-class classification; outputs probability distribution across multiple classes.
________________________________________
Q18. How do you visualize CNN filters or outputs?
A: You can use tools like matplotlib or TensorBoard to visualize:
‚Ä¢	Feature maps (activation maps)
‚Ä¢	Convolution filters
‚Ä¢	Class probabilities
________________________________________
Let me know if you want:
‚Ä¢	CNN code for MNIST using TensorFlow/Keras
‚Ä¢	Loss & accuracy plot
‚Ä¢	Confusion matrix visualization
I can also generate a ready-made Jupyter Notebook for your practical.

